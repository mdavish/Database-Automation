{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LPL Code Library\n",
    "---\n",
    "This workbook contains all the Python code written in the course of the _2018 LPL Mantas Rationalization_ engagement. The purpose of the engagement was to make improvements in LPL's Mantas AML Transaction monitoring system. In the engagement, Python was heavily utilized in two key workstreams:\n",
    "\n",
    "1. **Data Profiling:** The engagement began with an exploratory analysis of the Mantas database. On face value, the purpose of this exploration was to identify potential data points to be used in _segmentation_, the process of splitting the population into groups based on AML risk and transactional behavior. However, this exercise also proves useful in familiarizing the team with a large, idiosyncratic new database, and providing readily available summaries of the hundreds of fields in the database. In previous engagements, this process had involved the manual writing and execution of many SQL queries, but in this engagement that process was automated using Python, greatly reducing the team's workload and enhancing the scope of the exercise.\n",
    "2. **\"Below the Line\" Testing:** A later portion of the engagement involved the prescription of _threshold sets_ for LPL's current AML scenarios. A \"threshold set\" describes the point beyond which a certain type of transactional behavior appears suspicious and should be investigated for potential financial crime. EY recommended separate thresholds for each of the aforementioned segments, so that the transaction-monitoring system would be dynamic and take into account a customer's underlying characteristics in order to determine whether activity was or was not suspicious. (For example, transactions that would look suspicious for a regular person might not seem so suspicious for an ultra high net worth client.) In order to affirm the validity of these thresholds, \"below the line\" (BTL) testing was conducted to assess whether there was statistically significant suspicious acitivitiy going on *beneath* the recommended thresholds. If there had been, then the thresholds would have been lowered further. Python was used in this workstream to automate the collection and aggregation of random samples that would be large enough to determine, with stastical confidence, that there was no such activitiy, thereby validating the recommended thresholds.\n",
    "\n",
    "This workbook contains the code, scrubbed of client data, used to accomplish these two tasks, as well as detailed commentary and walkthroughts on the mechanisms of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: The pandas.tslib module is deprecated and will be removed in a future version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import cx_Oracle #For connecting to the database\n",
    "import pandas #For working with DataFrames and exporting to .csvs\n",
    "import numpy as np #For numerical calculations\n",
    "import time #For testing the speed/performance of our code\n",
    "import re #For using regular expression to text match\n",
    "import functools #For memoization\n",
    "import random #For setting the seed, extracting random samples\n",
    "from datetime import timedelta #For dynamic lookback periods\n",
    "\n",
    "from cx_Oracle import DatabaseError as DatabaseError\n",
    "from cx_Oracle import InterfaceError as InterfaceError\n",
    "from cx_Oracle import OperationalError as OperationalError\n",
    "from pandas.tslib import OutOfBoundsDatetime as OutOfBoundsDateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Database Interfacing\n",
    "---\n",
    "\n",
    "### Connecting to Oracle Database\n",
    "\n",
    "All code in this engagement required querying an Oracle 11g Database. To communicate with this type of database in Python, we utilize the [cx_Oracle library](https://oracle.github.io/python-cx_Oracle/), which is specific to Oracle databases but works much like any other Python database package (such as MySQL, PostgreSQL, etc.). \n",
    "\n",
    "These libraries work by creating a **cursor object** through which we can pass queries and other statements. Creating a cursor object requires connecting with proper credentials to the database. Sometimes the syntax for these connections can be confusing and not-straightforward, so we created a function that would make connection simple for the user and handle basic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Connect(user = 'Insert your username here', \n",
    "            pw = 'Insert your password here', \n",
    "            host = 'Insert your hostname here', \n",
    "            port = 'Insert your port here', \n",
    "            db='Insert the name of the database  here'):\n",
    "    \n",
    "    address = '%s/%s@%s:%s/%s' % (user, pw, host, port, db)\n",
    "    \n",
    "    #The code will attempt to connect as many as five times before failing.\n",
    "    #Failures are routine and can happen randomly, but five in a row means something is wrong.\n",
    "    tries = 0\n",
    "    while tries < 5:\n",
    "        tries += 1\n",
    "        try:\n",
    "            con = cx_Oracle.connect(address)\n",
    "            print('Connection Succesful!')\n",
    "            break\n",
    "        except DatabaseError:\n",
    "            print('Connection Failed - Trying Again...')\n",
    "\n",
    "    cur = con.cursor()   \n",
    "    return cur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes four basic arguments - `user`, `pw`, `host`, `port`, and `db`, all of which are self-explanatory and will usually be provided by the client. Furthermore, we found that creating a connection often fails for no particular reason - especially when you're working through a VPN - so we introduce error handling that tells the program to attempt to connect five times before giving up. \n",
    "\n",
    "We would save this connection to a **cursor object** (usually  named `cur`) which becomes the backbone of the following SQL query function (which is, in turn, the back bone of all other code in this library). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Queries\n",
    "\n",
    "Once we have a cursor object, we can send regular SQL queries to the database, just like we would in a regular SQL workbench ([such as Oracle SQL Developer, which we used in this engagement](https://www.oracle.com/database/technologies/appdev/sql-developer.html)) and then get their results in Python, in whichever form is most convenient.\n",
    "\n",
    "Again, the syntax for doing this in `cx_Oracle` is a little verbose and cumbersome, so we wrote a function a more user-friendly function that simply takes in the following arguments:\n",
    "\n",
    "- **`query`**: A simple string of the SQL query that we want to pass to the database. (If it works in SQLDeveloper, it will work here too.)\n",
    "- **`cur`**: A cursor object, which is produced with the `Connect()` function above\n",
    "- **`limit`**: A limit to the number of records the query return. The default is 20, so as to avoid writing millions of rows into memory. (Note that many SQL workbenches basically have a built-in limit, which is why they may seem like they're able to handle large queries very quickly. If you run `SELECT *` from a big table, it won't return all 10M+ rows at once - it will fetch them as needed. Python has no such feature, so we need to create one.\n",
    "- **`output`**: A variable that tells the function what we want to output. There are three possible values: 'standard', 'df', and 'metadata'. Respectively, they return a simple list of lists of the results, a Pandas dataframe of the results, and the _metadata_ of the results (i.e. the column names, datatypes, and other information). \n",
    "\n",
    "**Note:** Depending on what it's being used for, you may want to [**memoize**](https://en.wikipedia.org/wiki/Memoization) this function. Memoizing the function makes it remember the results of previous calls. So if you run the function using arguments it's already seen, it will simply return the value it returned last time, rather than than doing all the computational work it had to do to get the original result.\n",
    "\n",
    "On one hand, this can save a lot of time while you're testing or completing repetitive tasks. During the engagement, SQL queries proved time-consuming to run, and even more so for the results to flow across the VPN. So, in cases when we could be certain that the underlying data would not change, we would often memoize the function to promote efficiency.\n",
    "\n",
    "On the other hand, if you're working with data that you know will change, it doesn't make sense to memoize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Un-comment this line if you want to memoize the function.\n",
    "#@functools.lru_cache(maxsize=None)\n",
    "def SQL_Query(query, cur, limit = 20, output = 'standard'):\n",
    "    \n",
    "    #First, we'll make sure that we got a valid argument for the 'output' variable.\n",
    "    #If we haven't, we'll raise an error to guide the user to a correct inputs.\n",
    "    if output not in ('df','standard','metadata'):\n",
    "        raise ValueError(\"This is not a recognized output - try 'standard', 'df', or 'metadata'.\")\n",
    "    \n",
    "    #Next, we'll try to execute the query five times.\n",
    "    #(Again, failures are random and routine, but five in a row means something is wrong.)\n",
    "    tries = 0\n",
    "    while tries < 5:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            break\n",
    "\n",
    "        #If we find that the connection has dropped, which happens often,  we'll just reconnect to the database.\n",
    "        except (InterfaceError, OperationalError) as e:\n",
    "            print('Query Failed - Reconnecting')\n",
    "            #(These are the two types of errors that occur when the connection has dropped.)\n",
    "            \n",
    "            cur = Connect()\n",
    "            tries +=1\n",
    "    \n",
    "    #These are basic computations for extracting the names of each column returned\n",
    "    descriptions = cur.description\n",
    "    colnames = [field[0] for field in descriptions]\n",
    "    \n",
    "    #If we're returning only the metadata, all we need is the descriptions variable.\n",
    "    if output == 'metadata':\n",
    "        return descriptions\n",
    "    \n",
    "    #If we're returning the actual results of the query, we'll now need to adhere to the limit variable we defined:\n",
    "    elif output in ('df', 'standard'):\n",
    "        results = []\n",
    "        \n",
    "        #If there's no limit, capture all the results:\n",
    "        if limit is False:\n",
    "            results = [result for result in cur]\n",
    "\n",
    "        #If there is, capture only as many as instructed:\n",
    "        else:\n",
    "            results = [result for result in cur[:range(limit)]]\n",
    "    \n",
    "    #The 'standard\" output is just a list of lists...\n",
    "    if output == 'standard':\n",
    "        return results\n",
    "\n",
    "    #...whereas the 'df' output turns the lists into a neat dataframe, complete with column names.\n",
    "    elif output == 'df':\n",
    "        df = pandas.DataFrame(results)\n",
    "        df.columns = colnames\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Profiling\n",
    "---\n",
    "\n",
    "The EY data profiling methodology measures three things: completeness, correctness, and class differentiation. The idea is to make sure that no data is missing, that the data is correct, and then to explore how the data is differentiated, which we'd do differently depending on the type and distribution of the data. This function automates the measurement of completeness and class differentiation. (Measuring correctness is more complicated and can require cross-referencing source systems or testing individual datapoints.)\n",
    "\n",
    "Measuring **completeness** is simple - the function calculates what proportion data is `NULL`. Measuring **class differentiation** is more complicated, and depends on the data. The algorithm we wrote uses the following logic:\n",
    "- If the data is numerical, compute summary statistics\n",
    "- If the data is datetime, compute a date range\n",
    "- If the data is string and there are only a few distinct values, show which % belong to each category\n",
    "- If the data is string and has many values, just compute the # of unique values\n",
    "\n",
    "### Profile Field\n",
    "The first function conducts this analysis on a single field in a single table of a database. It takes the following arguments:\n",
    "- **`field`**: The name of the field we want to profile\n",
    "- **`table`**: The name of the table containing that field \n",
    "- **`schema`**: The name of the schema containing that table (which defaults to 'BUSINESS' - the schema most commonly used in the engagement)\n",
    "- **`cur`**: A cursor object, which will default to the predefined `cur` returned by the `Connect()` function\n",
    "\n",
    "(This function stayed memoized, as the underlying dataset was intentionally left static.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def Profile_Field(field, \n",
    "                  table, \n",
    "                  schema='BUSINESS', \n",
    "                  distinction_threshold = 0.3, \n",
    "                  max_distinct = 100, \n",
    "                  cur = cur):\n",
    "    \n",
    "    print('Computing Class Diff for %s' % field)\n",
    "    \n",
    "    #Step 1: Establish the size of the table\n",
    "    print('Establishing Size for %s' % field)\n",
    "    row_count_query = 'SELECT /*parallel (12)*/ COUNT(*) FROM %s.%s' % (schema, table)\n",
    "    row_count_results = SQL_Query(row_count_query, cur=cur)\n",
    "    row_count = row_count_results[0][0]\n",
    "    \n",
    "    #Step 2: Find the datatype\n",
    "    print('Finding MetaData for %s' % field)\n",
    "    dtype_query = 'SELECT %s FROM %s.%s' % (field, schema, table)\n",
    "    dtype_results = SQL_Query(dtype_query, cur=cur, output='metadata') \n",
    "    dtype = dtype_results[0][1]\n",
    "    \n",
    "    #Step 3: Find NULL Values\n",
    "    print('Finding NULL for %s' % field)\n",
    "    null_query = '''\n",
    "        SELECT /*parallel(12)*/\n",
    "            AVG(NULL_COUNT) AS PERCENT_NULL,\n",
    "            SUM(NULL_COUNT) AS NULL_COUNT,\n",
    "            1 - AVG(NULL_COUNT) AS PERCENT_COMPLETE\n",
    "        FROM (\n",
    "        SELECT \n",
    "            CASE WHEN %s IS NULL THEN 1 ELSE 0 END AS NULL_COUNT\n",
    "            FROM %s.%s\n",
    "        )\n",
    "        ''' % (field, schema, table)\n",
    "\n",
    "    null_query_result = SQL_Query(null_query, cur=cur)\n",
    "    \n",
    "    null_dict = {\n",
    "        'Table' : table,\n",
    "        'Field' : field,\n",
    "        'Percent_Null' : null_query_result[0][0],\n",
    "        'Null_Count' : null_query_result[0][1],\n",
    "        'Percent_Complete' : null_query_result[0][2]\n",
    "    }\n",
    "    \n",
    "    if null_query_result[0][0] == 1:\n",
    "        results_dict = {\n",
    "                'Table': table,\n",
    "                'Field': field,\n",
    "                'Dtype': str(dtype),\n",
    "                'Unique_Values' : 0\n",
    "        }\n",
    "        \n",
    "        return (dict(results_dict, **null_dict))\n",
    "   \n",
    "    #Step 4: Find the number of distinct values, unless it's all null\n",
    "    print('Finding Distinct Values %s' % field)\n",
    "    n_distinct_query = 'SELECT /*parallel (12)*/ COUNT(DISTINCT(%s)) FROM %s.%s' % (field, schema, table)\n",
    "    n_distinct_results = SQL_Query(n_distinct_query, cur=cur)\n",
    "    distinct_values = n_distinct_results[0][0]\n",
    "    \n",
    "    \n",
    "    #Step 5: Determine a course of action based on the distinct values, total values, and datatype\n",
    "    print('Assessing Class Diff for %s' % field)\n",
    "    \n",
    "    #First, we'll determine whether or not the field is too differentiated to compute a result (regardless of type).\n",
    "    #We'll use the previously defined DISTINCTION THRESHOLD and the MAX DISTINCT inputs.\n",
    "    \n",
    "    if (distinct_values / row_count) > distinction_threshold or distinct_values > max_distinct:\n",
    "        \n",
    "        #If there are too many values, our response will depend on the datatype of the field:\n",
    "        \n",
    "        #If the class is numeric, we want to compute summary statistics:\n",
    "        if dtype == cx_Oracle.NUMBER:\n",
    "            print('Finding Summary Stats for %s' % field)\n",
    "            summary_stats_query = \\\n",
    "                    '''\n",
    "                    SELECT /*parallel(12)*/\n",
    "                    MAX(%s) AS Max_Value, \n",
    "                    MIN(%s) AS Min_Value,\n",
    "                    AVG(%s) AS Mean,\n",
    "                    MEDIAN(%s) AS Median,\n",
    "                    STDDEV(%s) AS Standard_Deviation,\n",
    "                    PERCENTILE_CONT(0.85) WITHIN GROUP (ORDER BY %s) \"P85\",\n",
    "                    PERCENTILE_CONT(0.977) WITHIN GROUP (ORDER BY %s) \"P977\"\n",
    "                    FROM %s.%s''' % (field, field, field, field, field, field, field, schema, table)\n",
    "            \n",
    "            #We used to index this manually, but actually making it into a DataFrame is more scalable.\n",
    "            summary_stats_results = SQL_Query(summary_stats_query, cur=cur, output='df')\n",
    "            summary_stats_dict = summary_stats_results.to_dict(orient='records')\n",
    "            \n",
    "            results_dict = {\n",
    "                'Table': table,\n",
    "                'Field': field,\n",
    "                'Dtype': str(dtype),\n",
    "                'Unique_Values' : distinct_values,\n",
    "                'Summary_Stats' : summary_stats_dict\n",
    "            }\n",
    "            \n",
    "        #If the class is a date, we'll do something similar, except we'll only calculate the minimum and maximimum date:\n",
    "        elif dtype == cx_Oracle.DATETIME:\n",
    "            print('Finding DateRange for %s' % field)\n",
    "            try:\n",
    "                date_range_query = \\\n",
    "                        '''\n",
    "                        SELECT /*parallel(12)*/\n",
    "                        MAX(%s) AS Latest_Date, \n",
    "                        MIN(%s) AS First_Date\n",
    "                        FROM %s.%s''' % (field, field, schema, table)\n",
    "\n",
    "                date_range_results = SQL_Query(date_range_query, cur=cur, output='df')\n",
    "                date_range_dict = date_range_results.to_dict(orient='records')\n",
    "\n",
    "                results_dict = {\n",
    "                    'Table': table,\n",
    "                    'Field': field,\n",
    "                    'Dtype': str(dtype),\n",
    "                    'Unique_Values' : distinct_values,\n",
    "                    'Date_Range' : date_range_dict\n",
    "                }\n",
    "            \n",
    "            #It's possible that the datetime will be out of Pandas' interpretable range, in which case:\n",
    "            except OutOfBoundsDateTime:\n",
    "                print('Found Out-of-Bounds Date Range for %s' % field)\n",
    "                results_dict = {\n",
    "                    'Table': table,\n",
    "                    'Field': field,\n",
    "                    'Dtype': str(dtype),\n",
    "                    'Unique_Values' : distinct_values,\n",
    "                    'Date_Range' : 'Error - Date Out of Range'\n",
    "                }\n",
    "        \n",
    "        #If the class is neither a date nor a number, however, we don't want to compute anything at all:\n",
    "        else:\n",
    "            print('Nothing to Compute for %s' % field)\n",
    "            results_dict = {\n",
    "                'Table': table,\n",
    "                'Field': field,\n",
    "                'Dtype': str(dtype),\n",
    "                'Unique_Values' : distinct_values,\n",
    "                'Class_Diff' : 'Not Applicable'\n",
    "            }\n",
    "    \n",
    "    #If there were NOT too many values, we want to now show class differentiation regardless of datatype\n",
    "    else:\n",
    "        print('Finding Value Breakdown for %s' % field)\n",
    "        ##Old Query:\n",
    "        value_breakdown_query = \\\n",
    "                        '''SELECT /*parallel(12)*/\n",
    "                            %s, \n",
    "                        COUNT(*) AS NUM_RECORDS,\n",
    "                        COUNT(*) / (SELECT COUNT(*) FROM %s.%s) AS PERCENT_TOTAL\n",
    "                        FROM %s.%s GROUP BY %s''' \\\n",
    "                        % (field, schema, table, schema, table, field)\n",
    "                \n",
    "        #New Query:\n",
    "        value_breakdown_query = \\\n",
    "        '''\n",
    "            WITH COUNT_RECORD AS (\n",
    "                SELECT /*+ parallel(12)*/ \n",
    "                    COUNT(*) num\n",
    "                FROM\n",
    "                    %s.%s\n",
    "            ) SELECT /*+ parallel(12)*/ \n",
    "                %s,\n",
    "                COUNT(*) AS NUM_RECORDS,\n",
    "                COUNT(*) / max(COUNT_RECORD.num) AS PERCENT_TOTAL\n",
    "            FROM\n",
    "                %s.%s,\n",
    "                COUNT_RECORD\n",
    "            WHERE\n",
    "                1 = 1\n",
    "            GROUP BY\n",
    "                 %s\n",
    "        ''' % (schema, table, field, schema, table, field)\n",
    "        \n",
    "        try:\n",
    "            value_breakdown_results = SQL_Query(value_breakdown_query, cur=cur, limit = False, output = 'df')\n",
    "            value_breakdown_dict = value_breakdown_results.to_dict(orient='records')\n",
    "            #(See Pandas documentation for more info on the 'orien' argument and other possible values)\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            value_breakdown_dict = 'Error - NoneType'\n",
    "        \n",
    "        #Not a perfect representation of the data, but as good as we can hope for:  \n",
    "        \n",
    "        results_dict = {\n",
    "                'Table': table,\n",
    "                'Field': field,\n",
    "                'Dtype': str(dtype),\n",
    "                'Unique_Values' : distinct_values,\n",
    "                'Value_Breakdown' : value_breakdown_dict\n",
    "        }\n",
    "        \n",
    "    return (dict(results_dict, **null_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Table\n",
    "The next function profiles an entire _table_, using `Profile_Field` for each field within the table and then aggregating the results into a large dictionary. It takes the same arguments as `Profile_Field`, except for `field`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Profile_Table(table, schema='BUSINESS', cur=cur):\n",
    "    #Don't forget to record performance:\n",
    "    start_time = time.gmtime()\n",
    "    \n",
    "    fields_query = '''SELECT * FROM %s.%s''' % (schema, table)\n",
    "    fields_query_results = SQL_Query(fields_query, cur=cur, output='metadata')\n",
    "    col_names = [result[0] for result in fields_query_results]\n",
    "    \n",
    "    full_profile_list = [Profile_Field(column, table=table, schema=schema, cur=cur) for column in col_names]\n",
    "    \n",
    "    end_time = time.gmtime()\n",
    "    print(\"This took \" + str(time.mktime(end_time) - time.mktime(start_time)) + \" seconds.\")\n",
    "    \n",
    "    return full_profile_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Profile to CSV\n",
    "The last function sends the table's full profile to CSVs. Given the way the data profiling report was organized, there were four CSVs generated for each table:\n",
    "\n",
    "- A \"full profile\" which contained basic data on _all_ fields in the table, such as its type, number of unique values, and proportion of null values\n",
    "- A \"summary stats\" CSV containing the summary stats computed for all numeric fields\n",
    "- A \"date range\" CSV containing the ranges for all datetime fields\n",
    "- A \"class differentiation\" CSV containing the breakdowns for categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Profile_to_CSV(profile_output, table_name = 'Table'):\n",
    "    \n",
    "    def DType_RegEx(oracle_dtype):\n",
    "        pattern = re.compile(\"<class 'cx_Oracle\\.(\\w+)'>\")\n",
    "        match = re.search(pattern, oracle_dtype).group(1)\n",
    "        clean_dtype = match.title()\n",
    "        return clean_dtype\n",
    "    \n",
    "    #Step 1: Output the \"FullP Profile\"/Master CSV\n",
    "    profile_df = pandas.DataFrame(profile_output)\n",
    "    #Take only the relevant fields\n",
    "    profile_df = profile_df[['Table', 'Field', 'Dtype', 'Unique_Values', 'Null_Count','Percent_Complete']]\n",
    "    #Clean the \"Dtype\" field using the RegEx function\n",
    "    profile_df['Dtype'] = profile_df['Dtype'].apply(DType_RegEx)\n",
    "    #Write to CSV\n",
    "    profile_df.to_csv('%s_Full_Profile.csv' % table_name)\n",
    "    \n",
    "    summary_stats_list = []\n",
    "    date_range_list = []\n",
    "    value_breakdown_list = []\n",
    "    \n",
    "    for field in profile_output:\n",
    "        try:\n",
    "            #We need a lightweight dictionary to merge with subsequent dictionaries\n",
    "            mini_dict = {'Table': field['Table'], 'Field' : field['Field']}\n",
    "\n",
    "            if 'Summary_Stats' in field.keys():\n",
    "                adjusted_stats_dict = dict(mini_dict, **field['Summary_Stats'][0])\n",
    "                summary_stats_list.append(adjusted_stats_dict)\n",
    "\n",
    "            elif 'Date_Range' in field.keys():\n",
    "                if field['Date_Range'] == 'Error - Date Out of Range':\n",
    "                    print('Date Error on Field: %s' % field['Field'])\n",
    "\n",
    "                else:\n",
    "                    adjusted_range_dict = dict(mini_dict, **field['Date_Range'][0])\n",
    "                    date_range_list.append(adjusted_range_dict)\n",
    "\n",
    "            elif 'Value_Breakdown' in field.keys() and field['Percent_Null'] < 1:\n",
    "                #Remember, the values are dicts, but the keys are different every time!\n",
    "                for value in field['Value_Breakdown']:\n",
    "                    adjusted_value_dict = dict(mini_dict, **value)\n",
    "                    #Fort the love of God I hope this works\n",
    "                    adjusted_value_dict['Value'] = adjusted_value_dict.pop(field['Field'])\n",
    "                    value_breakdown_list.append(adjusted_value_dict)\n",
    "        except TypeError:\n",
    "            print('Error on %s' % field)\n",
    "            \n",
    "    pandas.DataFrame(summary_stats_list).to_csv('%s_Summary_Stats.csv' % table_name)\n",
    "    pandas.DataFrame(date_range_list).to_csv('%s_Date_Ranges.csv' % table_name)\n",
    "    #Don't forget to sort the value breakdown:\n",
    "    pandas.DataFrame(value_breakdown_list).sort_values(by=['Field','NUM_RECORDS'], \\\n",
    "                            ascending = [True, False]).to_csv('%s_Value_Breakdowns.csv' % table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: BTL Sampling\n",
    "---\n",
    "\n",
    "The last portion of the code exports below-the-line samples for AML scenarios. It should be noted that this part of the code is the least generalizable. Everything up to this point would work on any database in any context, and aren't really specific to AML or even financial services. But this final function is tailored to the particular structure of the tables and queries created during this engagement. It won't work for other engagements, but the code is still useful.\n",
    "\n",
    "The algorithm takes the following arguments:\n",
    "- **`scenario_name`**: The name of the scenario we're testing. (This is purely for labeling purposes and doesn't really affect the code.)\n",
    "- **`lookback`**: The \"lookback period\" of the scenario (i.e. the timeframe over which it monitors customers). The function needs to know this so that it can export transactions from the relevant timeframe.\n",
    "- **`scenario_focus`**: The \"focus\" of the scenario. (Some scenarios monitor individual accounts, while others monitor customers or entire households.) All of the queries in the function depend on this variable. \n",
    "- **`alert_field`** and **`rundate_field`**: The actual names of the fields in the tables that contain data on AML alerts. Sometimes these are spelled differently, so we needed to allow the function to specify them in cases when they were. \n",
    "- **`seed`**: Since this function involves some _random_ sampling, it was important to allow the seed to be configurable so that we could replicate results. (If you don't set the seed, you get different results every time.)\n",
    "\n",
    "Given these arguments, the function works like this:\n",
    "1. Find all of the \"below-the-line\" alerts in the relevant scenario over the past year.\n",
    "2. Using the hypergeometric function, compute how many alerts we'd need to investigate to determine with statistical confidence that there was an acceptable amount of suspicious activity going on below the line. (That computation rests on lots of assumptions like acceptable rate of suspicious activity, confidence interval, margin of error, etc.)\n",
    "3. Take a random sample of that size.\n",
    "4. For all the accounts, customers, or households in that sample, query all relevant customer data and transaction data for the relevant timeframe.\n",
    "5. Export all that data to CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BTL_Sample(scenario_name, \n",
    "               alerts_table, \n",
    "               lookback, \n",
    "               scenario_focus, \n",
    "               schema, \n",
    "               alert_field = 'ALERT_COUNT', \n",
    "               rundate_field = 'RUNDATE',\n",
    "               seed = 1234):\n",
    "\n",
    "    start_time = time.gmtime()\n",
    "    \n",
    "    #Step 1: Determine Population Size of Strata (Using SIG vs. Non-SIG as Strata)\n",
    "    if scenario_focus not in ('HH','ACCT'):\n",
    "        raise ValueError('HH and ACCT are the only valid scenario foci.')\n",
    "\n",
    "    #From the scenario focus, we can infer a few important parts of the query, so that it is made dynamic.\n",
    "    focus_variables = {\n",
    "        'HH' : {\n",
    "            'segmentation_table' : 'BUSINESS.EY_TRXN_SIG_HOUSEHOLD',\n",
    "            'primary_key' : 'ACCT_GRP_ID'\n",
    "        },\n",
    "        'ACCT' : {\n",
    "            'segmentation_table' : 'BUSINESS.EY_TRXN_SIG_MODEL2',\n",
    "            'primary_key' : 'ACCT_INTRL_ID'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print('Variables defined.')\n",
    "    \n",
    "    strata_pop_query = '''\n",
    "                        SELECT \n",
    "                            CASE WHEN EY.SIG IS NOT NULL THEN 'SIG' ELSE 'NON-SIG' END AS STRATA,\n",
    "                            COUNT(*) AS FULL_POP\n",
    "                        FROM\n",
    "                            {schema}.{alerts_table} AL INNER JOIN\n",
    "                            {segmentation_table} EY ON AL.{primary_key} = EY.{primary_key}\n",
    "                        WHERE\n",
    "                            {alert_field} = 0\n",
    "                        GROUP BY\n",
    "                            CASE WHEN EY.SIG IS NOT NULL THEN 'SIG' ELSE 'NON-SIG' END\n",
    "                        '''.format(schema = schema, \n",
    "                                   alerts_table = alerts_table, \n",
    "                                   alert_field = alert_field,\n",
    "                                   primary_key = focus_variables[scenario_focus]['primary_key'],\n",
    "                                   segmentation_table = focus_variables[scenario_focus]['segmentation_table'],\n",
    "                                  )\n",
    "    \n",
    "    print('Querying strata populations.')\n",
    "\n",
    "    strata_pop = SQL_Query(cur = cur,\n",
    "                           query = strata_pop_query,\n",
    "                           limit = False,\n",
    "                           output = 'standard'\n",
    "                          )\n",
    "    \n",
    "    #print(strata_pop)\n",
    "    \n",
    "    for result in strata_pop:\n",
    "        print(result[0] + ' - ' + str(result[1]))\n",
    "    \n",
    "    print('Results saved.')\n",
    "\n",
    "    #Step 3: Hypergeometric Function for Computing Sample Size\n",
    "    def required_size(N, ME=0.05 , p=0.05 , Z=1.64):\n",
    "        q = 1 - p\n",
    "        n = (N * (Z ** 2) * p * q) / ((ME ** 2) * (N - 1) + (Z ** 2) * p * q)\n",
    "        return n\n",
    "    \n",
    "    #Step 3.5: Based on those populations, determine the necessary sample size \n",
    "    #(We'll make this a loop, since we may want to change our definition of Strata in the future)\n",
    "    samples_master = []\n",
    "\n",
    "    for result in strata_pop:\n",
    "        temp = {\n",
    "            'strata' : result[0],\n",
    "            'required' : required_size(result[1]),\n",
    "            'total_pop' : result[1]\n",
    "        }\n",
    "        \n",
    "        print(temp['strata'] + ' needs ' + str(round(temp['required'],2)) + ' samples')\n",
    " \n",
    "        samples_master.append(temp)\n",
    "        \n",
    "    #Step 4: For each strata, extract a random set of BTL alerts\n",
    "    #(Set seed, so that results are replicable)\n",
    "    random.seed(seed)\n",
    "\n",
    "    print('Random seed set at ' + str(seed) + '.')\n",
    "    \n",
    "    for strata in samples_master:\n",
    "        print('Grabbing samples for ' + strata['strata'] + '.')\n",
    "        #First, query the full BTL population of each strata:\n",
    "        strata_btl_query = '''\n",
    "        SELECT \n",
    "            AL.*\n",
    "        FROM\n",
    "            {schema}.{alerts_table} AL INNER JOIN\n",
    "            {segmentation_table} EY ON AL.{primary_key} = EY.{primary_key}\n",
    "        WHERE\n",
    "            AL.{alert_field} = 0 AND\n",
    "            CASE WHEN EY.SIG IS NOT NULL THEN 'SIG' ELSE 'NON-SIG' END = '{strata}'\n",
    "        '''.format(schema = schema,\n",
    "                   alerts_table = alerts_table,\n",
    "                   alert_field = alert_field,\n",
    "                   strata = strata['strata'],\n",
    "                   primary_key = focus_variables[scenario_focus]['primary_key'],\n",
    "                   segmentation_table = focus_variables[scenario_focus]['segmentation_table']\n",
    "                  )\n",
    "\n",
    "        full_BTL_strata_pop = SQL_Query(cur = cur,\n",
    "                                        query = strata_btl_query,\n",
    "                                        limit = False,\n",
    "                                        output = 'df'\n",
    "                                       )\n",
    "\n",
    "        #Then, we generate random indices:\n",
    "        #We want X random numbers between 0 and Y where...\n",
    "        #X --> The required sample sized, based on...\n",
    "        #Y---> The population size\n",
    "        \n",
    "        #Sampling with replacement:\n",
    "        #rows = np.random.randint(low=1, high = strata['total_pop'], size = int(strata['required']))\n",
    "        \n",
    "        #Sampling without replacement:\n",
    "        rows = random.sample(list(full_BTL_strata_pop.index), int(strata['required']))\n",
    "\n",
    "        #Then we will use those random numbers to pull a sample:\n",
    "        random_strata_sample = full_BTL_strata_pop.iloc[rows,:]\n",
    "        \n",
    "        #Don't forget to order the sample by primary key then rundate\n",
    "        random_strata_sample.sort_values(by = [focus_variables[scenario_focus]['primary_key'], rundate_field])\n",
    "\n",
    "        #We'll save all results to memory, in the original dictionary.\n",
    "        strata['full_BTL_population'] = full_BTL_strata_pop\n",
    "        strata['random_BTL_sample'] = random_strata_sample\n",
    "        \n",
    "    #Step 5: For each of the random samples, extract all transactions from the lookback period\n",
    "    #(Regardless of whether the transaction was associated with this particular scenario)\n",
    "\n",
    "    for strata in samples_master:\n",
    "        print('Grabbing transactions for ' + strata['strata'] + '.')\n",
    "        \n",
    "        trxns_query = '''\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            BUSINESS.EY_ELIG_TRXN_BTL T\n",
    "        WHERE'''\n",
    "\n",
    "\n",
    "        for random_sample in zip(strata['random_BTL_sample'].loc[:,focus_variables[scenario_focus]['primary_key']],strata['random_BTL_sample'].loc[:,rundate_field]):\n",
    "            where_clause = '''\n",
    "            ({primary_key} = '{identifier}' AND TRXN_DATE BETWEEN TO_DATE('{end_date}') AND TO_DATE('{start_date}')) OR'''\\\n",
    "            .format(primary_key = focus_variables[scenario_focus]['primary_key'],\n",
    "                                       identifier = random_sample[0],\n",
    "                                       #Ensure formatting is correct for SQL\n",
    "                                       start_date = str(random_sample[1].strftime('%d-%b-%y')),\n",
    "                                        #Lookback period remains a dynamic parameter\n",
    "                                       end_date = str((random_sample[1] + timedelta(days = -lookback)) .strftime('%d-%b-%y'))\n",
    "                                      )\n",
    "            trxns_query += where_clause\n",
    "\n",
    "        #Trim query of final, superfluous 'OR' + order by date \n",
    "        trxns_query = trxns_query[:(len(trxns_query)-3)]\n",
    "        trxns_query += '''\n",
    "        ORDER BY \n",
    "            {primary_key},\n",
    "            TRXN_DATE DESC\n",
    "        '''.format(primary_key = focus_variables[scenario_focus]['primary_key'])\n",
    "        \n",
    "        #Send the query\n",
    "        trxns_results = SQL_Query(cur = cur, \n",
    "              query = trxns_query,\n",
    "              limit = False,\n",
    "              output = 'df'\n",
    "             )\n",
    "\n",
    "        #Add the results to the samples master\n",
    "        strata['trxns_sample'] = trxns_results\n",
    "        \n",
    "    #Step 6: Extract Customer Data for all related customers\n",
    "    print('Extracting customer data.')\n",
    "    \n",
    "    #Need to extract fields relevant to investigators, in user-friendly (non-Mantas) format\n",
    "    #Query will differ depending on the focus of the scenario:\n",
    "    \n",
    "    if scenario_focus == 'HH':\n",
    "        cust_query_stem = '''\n",
    "        SELECT \n",
    "            DISTINCT --Given the join structure, duplicate records are possible\n",
    "            HH.ACCT_GRP_ID,\n",
    "            C.CUST_INTRL_ID,\n",
    "            C.FULL_NM AS NAME,\n",
    "            C.BIRTH_DT AS DOB,\n",
    "            C.CUST_ADD_DT AS DATE_ADDED,\n",
    "            C.FNCL_PRFL_LAST_UPDT_DT AS LAST_UPDATE,\n",
    "            C.TAX_ID,\n",
    "            C.ANNL_INCM_BASE_AM AS EST_ANNUAL_INCOME,\n",
    "            C.NET_WRTH_BASE_AM AS EST_NET_WORTH,\n",
    "            C.LQD_NET_WRTH_BASE_AM AS EST_LIQUID_NET_WORTH,\n",
    "            CASE \n",
    "                WHEN C.MRTL_STAT_CD = 'U' THEN 'Unknown'\n",
    "                WHEN C.MRTL_STAT_CD = 'M' THEN 'Married'\n",
    "                WHEN C.MRTL_STAT_CD = 'D' THEN 'Divorced'\n",
    "                WHEN C.MRTL_STAT_CD = 'W' THEN 'Widowed'\n",
    "                WHEN C.MRTL_STAT_CD = 'U' THEN 'Single'\n",
    "            END AS MARTIAL_STATUS,\n",
    "            C.MPLYR_NM AS EMPLOYER,\n",
    "            C.OCPTN_NM AS OCCUPATION,\n",
    "            CASE WHEN C.RES_CNTRY_CD IS NOT NULL THEN 'Y' ELSE 'N' END AS FOREIGN_RESIDENT_FLAG,\n",
    "            C.WLTH_SRC_DSCR_TX AS WEALTH_SOURCE,\n",
    "            C.CUST_CDT_RTNG AS CREDIT_RATING,\n",
    "            C.CUST_CDT_RTNG_SRC AS CREDIT_RATING_SOURCE\n",
    "        FROM\n",
    "            BUSINESS.ACCT_GRP HH INNER JOIN\n",
    "            BUSINESS.ACCT A ON HH.ACCT_GRP_ID = A.HH_ACCT_GRP_ID INNER JOIN\n",
    "            BUSINESS.CUST C ON A.PRMRY_CUST_INTRL_ID = C.CUST_INTRL_ID\n",
    "        WHERE\n",
    "            HH.ACCT_GRP_ID IN \n",
    "        '''\n",
    "        \n",
    "        cust_query_end = '''\n",
    "                ORDER BY\n",
    "                HH.ACCT_GRP_ID\n",
    "                '''\n",
    "        \n",
    "    elif scenario_focus == 'ACCT':\n",
    "        cust_query_stem = '''\n",
    "        SELECT \n",
    "            A.ACCT_INTRL_ID,\n",
    "            C.CUST_INTRL_ID,\n",
    "            C.FULL_NM AS NAME,\n",
    "            C.BIRTH_DT AS DOB,\n",
    "            C.CUST_ADD_DT AS DATE_ADDED,\n",
    "            C.FNCL_PRFL_LAST_UPDT_DT AS LAST_UPDATE,\n",
    "            C.TAX_ID,\n",
    "            C.ANNL_INCM_BASE_AM AS EST_ANNUAL_INCOME,\n",
    "            C.NET_WRTH_BASE_AM AS EST_NET_WORTH,\n",
    "            C.LQD_NET_WRTH_BASE_AM AS EST_LIQUID_NET_WORTH,\n",
    "            CASE \n",
    "                WHEN C.MRTL_STAT_CD = 'U' THEN 'Unknown'\n",
    "                WHEN C.MRTL_STAT_CD = 'M' THEN 'Married'\n",
    "                WHEN C.MRTL_STAT_CD = 'D' THEN 'Divorced'\n",
    "                WHEN C.MRTL_STAT_CD = 'W' THEN 'Widowed'\n",
    "                WHEN C.MRTL_STAT_CD = 'U' THEN 'Single'\n",
    "            END AS MARTIAL_STATUS,\n",
    "            C.MPLYR_NM AS EMPLOYER,\n",
    "            C.OCPTN_NM AS OCCUPATION,\n",
    "            CASE WHEN C.RES_CNTRY_CD IS NOT NULL THEN 'Y' ELSE 'N' END AS FOREIGN_RESIDENT_FLAG,\n",
    "            C.WLTH_SRC_DSCR_TX AS WEALTH_SOURCE,\n",
    "            C.CUST_CDT_RTNG AS CREDIT_RATING,\n",
    "            C.CUST_CDT_RTNG_SRC AS CREDIT_RATING_SOURCE\n",
    "        FROM\n",
    "            BUSINESS.ACCT A INNER JOIN\n",
    "            BUSINESS.CUST C ON A.PRMRY_CUST_INTRL_ID = C.CUST_INTRL_ID\n",
    "        WHERE\n",
    "            A.ACCT_INTRL_ID IN\n",
    "        '''\n",
    "        \n",
    "        cust_query_end = '''\n",
    "                ORDER BY\n",
    "                A.ACCT_INTRL_ID\n",
    "                '''\n",
    "    \n",
    "    for strata in samples_master:\n",
    "        primary_key_filter_clause = '('\n",
    "        \n",
    "        #Loop through all of the different primary keys and turn them into a WHERE Clause\n",
    "        for primary_key in strata['random_BTL_sample'][focus_variables[scenario_focus]['primary_key']]:\n",
    "            primary_key_filter_clause += str('\\'' + primary_key + '\\', ')\n",
    "        \n",
    "        #Trim the string, integrate it into the full query\n",
    "        primary_key_filter_clause = primary_key_filter_clause[:(len(primary_key_filter_clause) - 2)]\n",
    "        primary_key_filter_clause += ')'\n",
    "        \n",
    "        #Combine the three components of the query\n",
    "        full_customer_query = cust_query_stem + primary_key_filter_clause + cust_query_end\n",
    "                                                       \n",
    "        #Send query to database\n",
    "        customer_data = SQL_Query(cur = cur, \n",
    "                                  query = full_customer_query,\n",
    "                                  limit = False,\n",
    "                                  output = 'df'\n",
    "                                 )\n",
    "        \n",
    "        strata['customer_data'] = customer_data\n",
    "        \n",
    "                                                       \n",
    "    #Step 7: Export the data to Excel.\n",
    "    print('Sending to CSV.')\n",
    "    \n",
    "    for strata in samples_master:\n",
    "        #Add another column to the main page explaining which strata the samples belong to\n",
    "        strata['random_BTL_sample']['strata'] = strata['strata']\n",
    "\n",
    "        #To CSV:\n",
    "        strata['random_BTL_sample'].to_csv(scenario_name + '_' + strata['strata'] + '_BTL_Samples.csv')\n",
    "        strata['trxns_sample'].to_csv(scenario_name + '_' + strata['strata'] + '_BTL_Samples_Transactions.csv')\n",
    "        strata['customer_data'].to_csv(scenario_name + '_' + strata['strata'] + 'Customer_Data.csv')\n",
    "        \n",
    "    end_time = time.gmtime()\n",
    "    print(\"This took \" + str(time.mktime(end_time) - time.mktime(start_time)) + \" seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
